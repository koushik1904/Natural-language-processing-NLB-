{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4x0OhOOpuHvenvB/PvBdx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koushik1904/Natural-language-processing-NLB-/blob/main/NLP_LAB06_2403a52057.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKQspByq1FBh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from collections import defaultdict, Counter\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi_R6CQI3TPy",
        "outputId": "f72a4deb-f772-4e5f-bd04-3120bc35f0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update column name if needed\n",
        "df = pd.read_csv(\"/content/twitter_dataset.csv\")\n",
        "\n",
        "# Display all columns to find the correct tweet text column\n",
        "print(\"Available columns in the DataFrame:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Replace 'YOUR_ACTUAL_TWEET_COLUMN_NAME' with the correct column name found above\n",
        "# For example, if the column is 'TweetContent', change it to df['TweetContent']\n",
        "tweets = df['Text'].dropna().tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLp2JmJ-3YqM",
        "outputId": "07be7d76-41aa-424d-8289-0d4e4b274aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns in the DataFrame:\n",
            "['Tweet_ID', 'Username', 'Text', 'Retweets', 'Likes', 'Timestamp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess tweet text\n",
        "def preprocess_tweet(tweet):\n",
        "    \"\"\"\n",
        "    This function cleans a tweet by:\n",
        "    1. Removing URLs\n",
        "    2. Removing user mentions (@username)\n",
        "    3. Converting text to lowercase\n",
        "    4. Removing extra spaces\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove URLs (http, https, www links)\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+\", \"\", tweet)\n",
        "\n",
        "    # Remove Twitter mentions (@username)\n",
        "    tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
        "\n",
        "    # Convert text to lowercase for uniformity\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Remove leading and trailing whitespaces\n",
        "    tweet = tweet.strip()\n",
        "\n",
        "    return tweet\n",
        "\n",
        "\n",
        "# Apply preprocessing function to all tweets\n",
        "# Each tweet in the list is cleaned one by one\n",
        "tweets = [preprocess_tweet(t) for t in tweets]\n"
      ],
      "metadata": {
        "id": "kITcJFA930hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store POS-tagged tweets\n",
        "tagged_sentences = []\n",
        "\n",
        "# Tokenize and POS-tag each preprocessed tweet\n",
        "for tweet in tweets:\n",
        "\n",
        "    # Split tweet into individual words/tokens\n",
        "    tokens = nltk.word_tokenize(tweet)\n",
        "\n",
        "    # Avoid empty tweets\n",
        "    if tokens:\n",
        "        # Assign POS tags using NLTK (acts as weak supervision)\n",
        "        pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "        # Store the tagged sentence\n",
        "        tagged_sentences.append(pos_tags)\n"
      ],
      "metadata": {
        "id": "RY_uE8q64CYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store transition counts: P(tag_i | tag_{i-1})\n",
        "transition_counts = defaultdict(Counter)\n",
        "\n",
        "# Dictionary to store emission counts: P(word | tag)\n",
        "emission_counts = defaultdict(Counter)\n",
        "\n",
        "# Counter to store total occurrences of each POS tag\n",
        "tag_counts = Counter()\n"
      ],
      "metadata": {
        "id": "tvTXqhqf4n8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through each POS-tagged sentence\n",
        "for sentence in tagged_sentences:\n",
        "\n",
        "    # Start symbol for each sentence\n",
        "    previous_tag = \"<START>\"\n",
        "    tag_counts[previous_tag] += 1\n",
        "\n",
        "    # Process each word-tag pair in the sentence\n",
        "    for word, tag in sentence:\n",
        "\n",
        "        # Count transition from previous tag to current tag\n",
        "        transition_counts[previous_tag][tag] += 1\n",
        "\n",
        "        # Count emission of word given the tag\n",
        "        emission_counts[tag][word] += 1\n",
        "\n",
        "        # Count total occurrences of the tag\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "        # Update previous tag\n",
        "        previous_tag = tag\n"
      ],
      "metadata": {
        "id": "4iFrHYIw4qZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store transition probabilities\n",
        "transition_probs = defaultdict(dict)\n",
        "\n",
        "# Convert transition counts into probabilities\n",
        "for prev_tag in transition_counts:\n",
        "\n",
        "    # Total transitions from previous tag\n",
        "    total_transitions = sum(transition_counts[prev_tag].values())\n",
        "\n",
        "    for curr_tag in transition_counts[prev_tag]:\n",
        "\n",
        "        # P(curr_tag | prev_tag)\n",
        "        transition_probs[prev_tag][curr_tag] = (\n",
        "            transition_counts[prev_tag][curr_tag] / total_transitions\n",
        "        )\n"
      ],
      "metadata": {
        "id": "kjF5FcOm4svX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store emission probabilities\n",
        "emission_probs = defaultdict(dict)\n",
        "\n",
        "# Convert emission counts into probabilities\n",
        "for tag in emission_counts:\n",
        "\n",
        "    # Total words emitted by the tag\n",
        "    total_emissions = sum(emission_counts[tag].values())\n",
        "\n",
        "    for word in emission_counts[tag]:\n",
        "\n",
        "        # P(word | tag)\n",
        "        emission_probs[tag][word] = (\n",
        "            emission_counts[tag][word] / total_emissions\n",
        "        )\n"
      ],
      "metadata": {
        "id": "QvNqEzI54v43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample transition probabilities\n",
        "print(\"Sample Transition Probabilities:\\n\")\n",
        "\n",
        "for prev_tag in list(transition_probs.keys())[:5]:\n",
        "    print(f\"{prev_tag} ‚Üí {dict(list(transition_probs[prev_tag].items())[:5])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0mr9M3w4x6E",
        "outputId": "e87b6426-603d-4cec-cf7f-e98922608c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Transition Probabilities:\n",
            "\n",
            "<START> ‚Üí {'NN': 0.507, 'RB': 0.0837, 'JJ': 0.1894, 'CD': 0.0106, 'PRP': 0.0165}\n",
            "NN ‚Üí {'RBS': 0.0007694584196507843, 'NN': 0.40899674459899377, '.': 0.2308848771825984, 'VBG': 0.0050902633915359576, 'DT': 0.010174607872151523}\n",
            "RBS ‚Üí {'JJ': 0.2922374429223744, 'IN': 0.045662100456621, '.': 0.1917808219178082, 'RB': 0.0547945205479452, 'VBP': 0.0319634703196347}\n",
            "JJ ‚Üí {'VBP': 0.012950210250472301, '.': 0.10992443171430312, 'JJ': 0.15540252300566762, 'NN': 0.570738619050521, 'RB': 0.04655981473581571}\n",
            "VBP ‚Üí {'CC': 0.0023619722468261, 'NNS': 0.006495423678771774, 'WDT': 0.0023619722468261, 'DT': 0.020076764098021848, 'NN': 0.35094970967424466}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counter to store word frequencies\n",
        "word_frequency = Counter()\n",
        "\n",
        "# Count word occurrences across all tagged sentences\n",
        "for sentence in tagged_sentences:\n",
        "    for word, tag in sentence:\n",
        "        word_frequency[word] += 1\n",
        "\n",
        "# Identify words that appear only once\n",
        "rare_words = [word for word, freq in word_frequency.items() if freq == 1]\n",
        "\n",
        "print(\"Total number of rare words:\", len(rare_words))\n",
        "print(\"Sample rare words:\", rare_words[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PRLNffa40Ey",
        "outputId": "38ecdb6f-58cc-4aa6-df2f-896756d6d5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rare words: 0\n",
            "Sample rare words: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(tokens, transition_probs, emission_probs, tag_counts):\n",
        "    \"\"\"\n",
        "    Applies the Viterbi algorithm to find the most probable\n",
        "    POS tag sequence for a given list of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # List to store Viterbi probability tables\n",
        "    V = [{}]\n",
        "\n",
        "    # Dictionary to store best tag paths\n",
        "    path = {}\n",
        "\n",
        "    # List of all possible POS tags\n",
        "    tags = list(tag_counts.keys())\n",
        "\n",
        "    # ----- Initialization Step -----\n",
        "    for tag in tags:\n",
        "        if tag == \"<START>\":\n",
        "            continue\n",
        "\n",
        "        # Transition probability from START to current tag\n",
        "        transition_p = transition_probs[\"<START>\"].get(tag, 1e-6)\n",
        "\n",
        "        # Emission probability of first word given the tag\n",
        "        emission_p = emission_probs[tag].get(tokens[0], 1e-6)\n",
        "\n",
        "        # Store log probability to avoid underflow\n",
        "        V[0][tag] = math.log(transition_p) + math.log(emission_p)\n",
        "\n",
        "        # Initialize path\n",
        "        path[tag] = [tag]\n",
        "\n",
        "    # ----- Recursion Step -----\n",
        "    for t in range(1, len(tokens)):\n",
        "        V.append({})\n",
        "        new_path = {}\n",
        "\n",
        "        for current_tag in tags:\n",
        "            if current_tag == \"<START>\":\n",
        "                continue\n",
        "\n",
        "            # Emission probability for current word\n",
        "            emission_p = emission_probs[current_tag].get(tokens[t], 1e-6)\n",
        "\n",
        "            # Find best previous tag\n",
        "            (probability, best_prev_tag) = max(\n",
        "                (\n",
        "                    V[t-1][prev_tag]\n",
        "                    + math.log(transition_probs[prev_tag].get(current_tag, 1e-6))\n",
        "                    + math.log(emission_p),\n",
        "                    prev_tag\n",
        "                )\n",
        "                for prev_tag in V[t-1]\n",
        "            )\n",
        "\n",
        "            # Store best probability and path\n",
        "            V[t][current_tag] = probability\n",
        "            new_path[current_tag] = path[best_prev_tag] + [current_tag]\n",
        "\n",
        "        path = new_path\n",
        "\n",
        "    # ----- Termination Step -----\n",
        "    best_final_tag = max(V[-1], key=V[-1].get)\n",
        "\n",
        "    # Return word-tag pairs\n",
        "    return list(zip(tokens, path[best_final_tag]))\n"
      ],
      "metadata": {
        "id": "Uezv_CGu42SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample noisy tweet\n",
        "test_tweet = \"love this movie üòç\"\n",
        "\n",
        "# Preprocess and tokenize the tweet\n",
        "test_tokens = nltk.word_tokenize(preprocess_tweet(test_tweet))\n",
        "\n",
        "# Apply Viterbi decoding\n",
        "viterbi_result = viterbi(\n",
        "    test_tokens,\n",
        "    transition_probs,\n",
        "    emission_probs,\n",
        "    tag_counts\n",
        ")\n",
        "\n",
        "# Display final POS tags\n",
        "print(\"Viterbi POS Tagging Result:\\n\")\n",
        "for word, tag in viterbi_result:\n",
        "    print(f\"{word} / {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tDcW3ky45OQ",
        "outputId": "a0bb254f-4f5c-4e78-d37f-36934a3098e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Viterbi POS Tagging Result:\n",
            "\n",
            "love / NN\n",
            "this / DT\n",
            "movie / NN\n",
            "üòç / NN\n"
          ]
        }
      ]
    }
  ]
}